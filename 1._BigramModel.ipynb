{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Experimenting with Text files\n",
    "The model we will be using for this 1st section is called a [Bigram](https://web.stanford.edu/~jurafsky/slp3/3.pdf) model which is a type of Natural language processing (NLP) model that predicts a word based on the immediately preceding word. \n",
    "\n",
    "\n",
    "Text file used is the book Wizard of OZ which you can download from Gutenberg library for free.\n",
    "<br>Click link and make sure you select \"Plain Text UTF-8\"\n",
    "<br>https://www.gutenberg.org/ebooks/22566\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# Bring in text file \"Wizard of OZ\"\n",
    "with open('data/wizard_of_oz.txt', 'r', encoding='utf=8') as f:\n",
    "  text = f.read()\n",
    "# print(text[:200])\n",
    "# bring in all our uniqye text characters as a set and sort\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(len(chars)) # 81 unique character values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch - Deep Learning framework used to train neural networks\n",
    "Will allow us to train our model with GPU CUDA tensors \n",
    "https://pytorch.org/tutorials/beginner/basics/intro.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU CUDA is enabled otherwise use CPU \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Comparison - Pytorch Tensors vs Numpy Arrays for working with multidimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA: 0.01599765s\n",
      "Numpy: 0.16937160s\n"
     ]
    }
   ],
   "source": [
    "# Speed Check - Pytorch Cuda on GPU vs Numpy on CPU\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "np_rand2 = torch.rand(100, 100, 100, 100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Pytorch CUDA: {elapsed_time:.8f}s')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Numpy: {elapsed_time:.8f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing our text w Encoders and Decoders \n",
    "After gathering all the unique characters in our text we will need to convert these values into tokens. For this we need Encoders and Decoders <br>\n",
    "**Encoders**: Converts our text values into integers (makes it machine readable)<br>\n",
    "**Decoders**: Converts our integers into text values (makes it human readable after our model completes it's training)\n",
    "\n",
    "https://www.datacamp.com/blog/what-is-tokenization#:~:text=Imagine%20you%27re%20trying,the%20two%20contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "# Encoder and Decoder logic\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# encoded_hello = encode('hello')\n",
    "# decoded_hello = decode(encoded_hello)\n",
    "# print(f'Encoded hello =', encoded_hello)\n",
    "# print(f'Decoded hello =', decoded_hello)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping our characters into block sizes then chunking our blocks into batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([80]) prediction is tensor(1)\n",
      "When input is tensor([80,  1]) prediction is tensor(1)\n",
      "When input is tensor([80,  1,  1]) prediction is tensor(28)\n",
      "When input is tensor([80,  1,  1, 28]) prediction is tensor(39)\n",
      "When input is tensor([80,  1,  1, 28, 39]) prediction is tensor(42)\n",
      "When input is tensor([80,  1,  1, 28, 39, 42]) prediction is tensor(39)\n",
      "When input is tensor([80,  1,  1, 28, 39, 42, 39]) prediction is tensor(44)\n",
      "When input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) prediction is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "# Create block size, think of this as the words we want to be chunked together\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print('When input is', context, 'prediction is', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a train/test split (80/20)\n",
    "We will divide our data into two parts: 80% of the data will be used for training our model and 20% of the data will be used for testing, which is data unseen to our model.<br>\n",
    "https://builtin.com/data-science/train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  4383,  55969,  91389, 130744])\n",
      "inputs: \n",
      "tensor([[16,  1,  1, 44, 32, 29,  1, 46],\n",
      "        [73, 54, 67, 73,  9,  1, 73, 74],\n",
      "        [ 9,  1, 54, 67, 57,  1, 76, 58],\n",
      "        [72, 76, 68, 71, 57, 10, 55, 65]], device='cuda:0')\n",
      "Predictions: \n",
      "tensor([[ 1,  1, 44, 32, 29,  1, 46, 29],\n",
      "        [54, 67, 73,  9,  1, 73, 74, 56],\n",
      "        [ 1, 54, 67, 57,  1, 76, 58,  1],\n",
      "        [76, 68, 71, 57, 10, 55, 65, 54]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 80/20 Split\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else test_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  print(ix)\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('Predictions: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Bi-gram model and defining a Forward pass function\n",
    "Forward Pass - This process involves passing input data through set of layers called neural nets and applying math transformations (using weights, bias, activation functions) that help the model learn and identify patterns or relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embeddings_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward_pass(self, index, targets):\n",
    "    logits = self.token_embeddings_table(index)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
