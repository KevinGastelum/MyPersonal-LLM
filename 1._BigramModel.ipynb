{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Experimenting with Text files\n",
    "The model we will be using for this 1st section is called a [Bigram](https://web.stanford.edu/~jurafsky/slp3/3.pdf) model which is a type of Natural language processing (NLP) model that predicts a word based on the immediately preceding word. \n",
    "\n",
    "\n",
    "Text file used is the book Wizard of OZ which you can download from Gutenberg library for free.\n",
    "<br>Click link and make sure you select \"Plain Text UTF-8\"\n",
    "<br>https://www.gutenberg.org/ebooks/22566\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# Bring in text file \"Wizard of OZ\"\n",
    "with open('data/wizard_of_oz.txt', 'r', encoding='utf=8') as f:\n",
    "  text = f.read()\n",
    "# print(text[:200])\n",
    "# bring in all our uniqye text characters as a set and sort\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(len(chars)) # 81 unique character values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch - Deep Learning framework used to train neural networks\n",
    "Will allow us to train our model with GPU CUDA tensors \n",
    "https://pytorch.org/tutorials/beginner/basics/intro.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU CUDA is enabled otherwise use CPU \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Comparison - Pytorch Tensors vs Numpy Arrays for working with multidimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA: 0.01200151s\n",
      "Numpy: 0.27162862s\n"
     ]
    }
   ],
   "source": [
    "# Speed Check - Pytorch Cuda on GPU vs Numpy on CPU\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "torch_rand1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "torch_rand2 = torch.rand(100, 100, 100, 100).to(device)\n",
    "np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "np_rand2 = torch.rand(100, 100, 100, 100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Pytorch CUDA: {elapsed_time:.8f}s')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Numpy: {elapsed_time:.8f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing our text w Encoders and Decoders \n",
    "After gathering all the unique characters in our text we will need to convert these values into tokens. For this we need Encoders and Decoders <br>\n",
    "**Encoders**: Converts our text values into integers (makes it machine readable)<br>\n",
    "**Decoders**: Converts our integers into text values (makes it human readable after our model completes it's training)\n",
    "\n",
    "https://www.datacamp.com/blog/what-is-tokenization#:~:text=Imagine%20you%27re%20trying,the%20two%20contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([78,  1,  1, 27, 38, 41, 38, 43, 31, 47,  1, 24, 37, 27,  1, 43, 31, 28,\n",
      "         1, 46, 32, 48, 24, 41, 27,  1, 38, 29,  1, 38, 48,  0,  0,  1,  1, 25,\n",
      "        47,  0,  0,  1,  1, 35, 10,  1, 29, 41, 24, 37, 34,  1, 25, 24, 44, 36,\n",
      "         0,  0,  1,  1, 24, 44, 43, 31, 38, 41,  1, 38, 29,  1, 43, 31, 28,  1,\n",
      "        46, 32, 48, 24, 41, 27,  1, 38, 29,  1, 38, 48,  8,  1, 43, 31, 28,  1,\n",
      "        35, 24, 37, 27,  1, 38, 29,  1, 38, 48])\n"
     ]
    }
   ],
   "source": [
    "# Encoder and Decoder logic\n",
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "# encoded_hello = encode('hello')\n",
    "# decoded_hello = decode(encoded_hello)\n",
    "# print(f'Encoded hello =', encoded_hello)\n",
    "# print(f'Decoded hello =', decoded_hello)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping our characters now integers into block sizes then chunking our blocks into batch sizes\n",
    "After converting our text characters into integers we want to group our values into blocks then we group those blocks into batch sizes\n",
    "Think of blocks as words and batches as multiple words strung together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([78]) prediction is tensor(1)\n",
      "When input is tensor([78,  1]) prediction is tensor(1)\n",
      "When input is tensor([78,  1,  1]) prediction is tensor(27)\n",
      "When input is tensor([78,  1,  1, 27]) prediction is tensor(38)\n",
      "When input is tensor([78,  1,  1, 27, 38]) prediction is tensor(41)\n",
      "When input is tensor([78,  1,  1, 27, 38, 41]) prediction is tensor(38)\n",
      "When input is tensor([78,  1,  1, 27, 38, 41, 38]) prediction is tensor(43)\n",
      "When input is tensor([78,  1,  1, 27, 38, 41, 38, 43]) prediction is tensor(31)\n"
     ]
    }
   ],
   "source": [
    "# Create block size, think of this as the words we want to be chunked together\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "  context = x[:t+1]\n",
    "  target = y[t]\n",
    "  print('When input is', context, 'prediction is', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a train/test split (80/20)\n",
    "We will divide our data into two parts: 80% of the data will be used for training our model and 20% of the data will be used for testing, which is data unseen to our model.<br>\n",
    "https://builtin.com/data-science/train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 18657,  82894, 157116, 154562])\n",
      "inputs: \n",
      "tensor([[56, 63, 66, 74,  1, 71, 59, 56],\n",
      "        [53, 63, 56,  1, 67, 52, 71, 59],\n",
      "        [ 1, 51, 53, 56, 52, 72,  5, 57],\n",
      "        [ 1, 53, 56, 76, 66, 65, 55,  8]], device='cuda:0')\n",
      "Predictions: \n",
      "tensor([[63, 66, 74,  1, 71, 59, 56, 64],\n",
      "        [63, 56,  1, 67, 52, 71, 59, 70],\n",
      "        [51, 53, 56, 52, 72,  5, 57, 72],\n",
      "        [53, 56, 76, 66, 65, 55,  8,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 80/20 Split\n",
    "# n = int(0.8*len(data))\n",
    "# train_data = data[:n]\n",
    "# test_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "  data = train_data if split == 'train' else test_data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  print(ix)\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('Predictions: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Bi-gram model and defining a Forward pass function\n",
    "Forward Pass - This process involves passing input data through set of layers called neural nets and applying math transformations (using weights, bias, activation functions) that help our model learn and identify patterns or relationships in the data.\n",
    "\n",
    "### Probability dstribution of next-token predictions for English characters<br>\n",
    "<img src=\"data/next-token_probability_distribution.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nN(buZ? ssTI,b1([RlBhsnb\\n-05D\\nSzL\\'mN2j_TjxD[lNRB&HF);JN2WH,I-ilE0BSFvrOsHB,c3lvw.kc\"_&lW(jEYP5\\n]7u\\ufeffu2qZ[ChrBdIl&jv2LM.)tso?DStp]w) ,v:.p.HEWP?MbyTe\\nAJaz_RYq11SY?5M]s6gnef;-E0y\\ufeff\\ufeffm\\ufeff;h&&?-\"NJJwnx\\ufeffZK\\'_3z\\njTEA!-O.mkHYo5\"cIN?M1;([3fsYG?:Yail&EW yC\\ufeff)d2bA&7P,[kdYi yCYG? \"lPv?ZaPV7FZD1c\"5CfT\\ufeffqA&jU]GVDC4yRmCaRxnm2IgT?ZYYaq0]OQo_8auOKFi:Y)_H,,iwAB!m_GsL&k1?Z2:N25\"BNrk]1\\ufeffZ-jclY3oEuWwTh:]Vx4jLD,,E]L4BvHYeS \"f;n\\'Nq1&u?A!;4?Wss6N[JBNQuf4]1GM\\ufeffhF,E3?DtcG1hh5E9?\\nA&JNo\\'R4aSWif\\'J_w(ediuf;!&&Z\\ufeffTaB5tAqv[jzGJwc]ON2t4Zn'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, index, targets=None):\n",
    "      logits = self.token_embedding_table(index)\n",
    "\n",
    "      if targets is None:\n",
    "        loss = None\n",
    "      else:\n",
    "          # B = batches, T = targets, C = channels\n",
    "          B, T, C = logits.shape\n",
    "          logits = logits.view(B*T, C)\n",
    "          targets = targets.view(B*T)\n",
    "          loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "      return logits, loss\n",
    "  \n",
    "  def generate(self, index, max_new_tokens):\n",
    "    # index is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # get the predictions\n",
    "      logits, loss = self.forward(index)\n",
    "      # focus only on the last time step\n",
    "      logits = logits[:, -1, :] # becomes (B, C)\n",
    "      # apply softmax to get probabilities\n",
    "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "      # sample form the distribution\n",
    "      index_next = torch.multinomial(probs, num_samples=1)# (B, 1)\n",
    "      # append sampled index to the running sequence\n",
    "      index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "    return index\n",
    "  \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "generated_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
